{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# POS-tagging with Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reminder: POS-tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pos](pos.jpg)\n",
    "\n",
    "([SLP Ch.8](https://web.stanford.edu/~jurafsky/slp3/8.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Train** a model using a large **corpus** of **annotated** text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "_For each word $w$, choose the most frequent POS-tag in the training data_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\operatorname{POS}(w) = \\underset{l \\in L}{\\operatorname{argmax}} \\mathbb{P}(l|w) $$\n",
    "\n",
    "Where $L$ is the set of tags $l_1, l_2, \\ldots$ and $$\\mathbb{P}(l|w) \\sim \\frac{\\#(w, l)}{\\#w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Context matters too!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Remember **ngrams** ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ l_i = \\underset{l \\in L}{\\operatorname{argmax}} \\mathbb{P}(l|(l_{i-1}, l_{i-2})) \\sim \\frac{\\#(l_i, l_{i-1}, l_{i-2})}{\\#(l_{i-1}, l_{i-2})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We need to combine the two models of __likelihood__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hidden Markov Model (HMM)\n",
    "<a id=\"hmm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We need the most likely tag sequence for a given sequence of words:\n",
    "\n",
    "$$\n",
    "\\underset{l_i, l_j, \\ldots, l_n \\in L^N}{\\operatorname{argmax}}\\mathbb{P}(l_1, l_2, \\ldots l_N \\ | \\ w_1, w_2 \\ldots w_N)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But we'll be looking for this:\n",
    "$$\n",
    "\\underset{l_i, l_j, \\ldots, l_n \\in L^N}{\\operatorname{argmax}}\\mathbb{P}(w_1, w_2, \\ldots w_N \\ | \\ l_1, l_2 \\ldots l_N)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Could we possibly approximate this as\n",
    "$$\n",
    "    \\underset{l_i, l_j, \\ldots, l_n \\in L^N}{\\operatorname{argmax}} \\frac{\\#((l_i, l_j, \\ldots, l_n), (w_i, w_j, \\ldots, w_n))}{\\#(l_i, l_j, \\ldots, l_n)}\n",
    "$$\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's simplify a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the current POS tag depends only on a fixed window of $n$ past tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- word forms depend on their own POS tags only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### For $n=2$, let's optimize:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\mathbb{P}(w_1, w_2, \\ldots w_N \\ | \\ l_1, l_2 \\ldots l_N) =\n",
    "    \\prod_{i=1}^N\\mathbb{P}(l_i \\ |\\ l_{i-1})\\cdot\\mathbb{P}(w_i \\ | \\ l_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The terms $\\mathbb{P}(l_i \\ |\\ l_{i-1})$ are called **transition probabilities**\n",
    "* The terms $\\mathbb{P}(w_i\\ |\\ l_i)$ are called **emission or observation probabilities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### These we can approximate based on the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\mathbb{P}(l_n\\ |\\ l_{n-1})\\approx \\frac{\\#(l_{n-1},l_n)}{\\#l_{n-1}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "\n",
    "$$ \\mathbb{P}(w_i|l_i) \\approx \\frac{\\#(w_i, l_i)}{\\#l_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### So how do we calculate the most likely tagset?\n",
    "\n",
    "$$\n",
    "\\underset{l_i, l_j, \\ldots, l_n \\in L^N}{\\operatorname{argmax}} \\prod_{i=1}^N\\mathbb{P}(l_i \\ |\\ l_{i-1})\\cdot\\mathbb{P}(w_i \\ | \\ l_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Viterbi algorithm\n",
    "<a id=\"viterbi\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "| |the|dog|saw|a|cat|\n",
    "|-|---|---|---|-|---|\n",
    "|**DET**| |   |   | |   |\n",
    "|**NOUN**||   |   | |   |\n",
    "|**VERB**||   |   | |   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Viterbi algorithm\n",
    "<a id=\"viterbi\"></a>\n",
    "(for $n=2$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Input:\n",
    "* the sentence $W = \\{w_1, \\ldots,w_N\\}$\n",
    "* the hidden state space $S=\\{s_1, \\ldots , s_{|L|}\\}$ (the POS tags)\n",
    "* the transition probabilities $T_{i,j} = \\mathbb{P}(l_t=s_j|l_{t-1}=s_i)$\n",
    "* the emission probabilities $E_{i,j} = \\mathbb{P}(w_j|l_t=s_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "| |the|dog|saw|a|cat|\n",
    "|-|---|---|---|-|---|\n",
    "|**DET**| |   |   | |   |\n",
    "|**NOUN**||   |   | |   |\n",
    "|**VERB**||   |   | |   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> For each word $w_i$ and for each possible label $l_j$<br/>\n",
    "> $\\quad$ calculate the probability of the most likely sequence ending in $(w_i, l_j)$<br/>\n",
    "> $\\quad$ $\\pi[i,j] = \\underset{k \\in \\{1, \\ldots, |L|\\}}{\\operatorname{max}} \\pi[k,j-1]\\cdot T_{k,i} \\cdot E_{i,j}$<br/>\n",
    "> $\\quad$ and make a note of the previous state for that sequence<br/>\n",
    ">   $\\quad$ $B[i,j]=\\underset{k \\in \\{1, \\ldots, |L|\\}}{\\operatorname{argmax}} \\pi[k,j-1] \\cdot T_{k,i} \\cdot E_{i,j}$<br/>\n",
    "> Finally, use the backpointers in $B$ to read out the most likely sequence of POS-tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![viterbi](viterbi.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\mathcal{O}(N \\cdot |L|^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How is the transition probability calculated for the first tag?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How would the algorithm change for $n=3$ ? How does this change the time complexity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What could we do about words unseen in the training data? And unseen label sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### HMMs work for all kinds of sequence labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![ner](ner_70.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "| _American_ | _Airlines_ | , | _a_  | _unit_ | _of_ | _AMR_   | _Corp_  | _._     | _,_  | _immediately_ | _matched_ | _the_ | _move_ |\n",
    "| :-------: | :-------: | :-:| :-: | :---: | :-: | :--:   | :---:  | :-:    | :-: | :-: | :-: | :-: | :-: |\n",
    "| B-ORG    | I-ORG    | O | O  | O    | O  | B-ORG | I-ORG | I-ORG | O  | O  | O  | O  | O  |                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NP-chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "| _American_ | _Airlines_ | , | _a_  | _unit_ | _of_ | _AMR_   | _Corp_  | _._     | _,_  |\n",
    "| :-------: | :-------: | :-:| :-: | :---: | :-: | :--:   | :---:  | :-:    | :-: | \n",
    "| B-NP    | I-NP    | O | B-NP  | I-NP  | I-NP  | I-NP | I-NP | I-NP | O  | \n",
    "\n",
    "| _immediately_ | _matched_ | _the_ | _move_ |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| O  | O  | B-NP  | I-NP  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is sometimes called __shallow parsing__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Questions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can implement your own HMM for POS-tagging with [this notebook](https://github.com/bmeaut/python_nlp_2018_spring/blob/master/homeworks/homework2/homework2.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
